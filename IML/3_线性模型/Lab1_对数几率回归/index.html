<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>IML Lab1 report - Notes Site</title>
    <link href="../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "IML Lab1 report", url: "#_top", children: [
              {title: "Aims", url: "#aims" },
              {title: "Methods", url: "#methods" },
              {title: "Result", url: "#result" },
          ]},
        ];

    </script>
    <script src="../../../js/base.js"></script>
      <script src="../../../mathjax-config.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav">
      <a href="../3_%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../3_%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/" class="btn btn-xs btn-link">
        3 线性判别分析
      </a>
    </div>
    
  </div>

    

    <h1 id="iml-lab1-report">IML Lab1 report</h1>
<p><strong>Author</strong>: Ruixuan Huang     <strong>Student ID</strong>: PB20111686</p>
<div class="toc">
<ul>
<li><a href="#iml-lab1-report">IML Lab1 report</a><ul>
<li><a href="#aims">Aims</a></li>
<li><a href="#methods">Methods</a></li>
<li><a href="#result">Result</a><ul>
<li><a href="#loss-curve">Loss curve</a></li>
<li><a href="#comparation-table">Comparation table</a><ul>
<li><a href="#pnr">PNR</a></li>
<li><a href="#tar">TAR</a></li>
<li><a href="#lr">lr</a></li>
</ul>
</li>
<li><a href="#best-accuracy">Best accuracy</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h2 id="aims">Aims</h2>
<ol>
<li>Implement logistic regression framework using given dataset</li>
<li>Use the given dataset to test the model we trained</li>
<li>Illustrate loss curve of one training process</li>
<li>Give comparation table of different parameters</li>
<li>Give the best accuracy of test data</li>
</ol>
<h2 id="methods">Methods</h2>
<p><strong>Step1</strong>. Data process</p>
<p>The original dataset (<code>loan.csv</code>) is typical for dichotomous classification problem. Before using it to train models, I noted that there are some cases with missing attributes. Here my choice is using <code>df.dropna()</code> to discard these cases.</p>
<p>Then, we should convert non numerical descriptions of some attributes to continuous numerical descriptions. I noted that the values of each to-be-converted attribute have some order relationship. So I simply use <code>df[{attr_name}].map()</code> to convert.</p>
<p>I decided to use hold-out method to train a model. So my strategy to split the dataset can be described as follows:</p>
<ol>
<li>Use <code>df.sample(frac=1.0)</code> to randomize the dataset</li>
<li>Use <code>df.sort_values("Loan_Status", inplace=True)</code> to place the positive and negative cases in two consecutive areas to facilitate the data spliting</li>
<li>In the positive area, choose its top 40% cases and add them into <code>X_train</code> and <code>y_train</code>. For negative cases the rate is 80%. This is because the proportion of positive and negative of original dataset is 2:1</li>
<li>After executing the above 3 procedures, I get the <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, <code>y_test</code></li>
</ol>
<p><strong>Step2</strong>. Implement the LR class</p>
<p>According to the (3.27) in the textbook, our objective is to minimize</p>
<div class="arithmatex">
<div class="MathJax_Preview">
l(\boldsymbol{\beta})=\sum_{i=1}^m\left(-y_i\boldsymbol{\beta}^\top\hat{\boldsymbol{x}}_i+\ln\left(1+e^{\boldsymbol{\beta}^\top\hat{\boldsymbol{x}}_i}\right)\right)
</div>
<script type="math/tex; mode=display">
l(\boldsymbol{\beta})=\sum_{i=1}^m\left(-y_i\boldsymbol{\beta}^\top\hat{\boldsymbol{x}}_i+\ln\left(1+e^{\boldsymbol{\beta}^\top\hat{\boldsymbol{x}}_i}\right)\right)
</script>
</div>
<p>I assumed that <code>fit_intercept=True</code>, so here <span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{\beta}=(\boldsymbol{w};b),\hat{\boldsymbol{x}}=(\boldsymbol{x};1)</span><script type="math/tex">\boldsymbol{\beta}=(\boldsymbol{w};b),\hat{\boldsymbol{x}}=(\boldsymbol{x};1)</script></span>.</p>
<blockquote>
<p>In my code I also dealt with the situation when <code>fit_intercept=False</code>, but in the report I'll only discuss the situation when <code>fit_intercept=True</code>.</p>
</blockquote>
<p>I use <span class="arithmatex"><span class="MathJax_Preview">\displaystyle\sigma(x)=\frac{1}{1+e^{-z}}</span><script type="math/tex">\displaystyle\sigma(x)=\frac{1}{1+e^{-z}}</script></span> as the sigmoid function, so the result of one prediction is <span class="arithmatex"><span class="MathJax_Preview">\sigma(\boldsymbol{\beta}^\top\hat{\boldsymbol{x}})</span><script type="math/tex">\sigma(\boldsymbol{\beta}^\top\hat{\boldsymbol{x}})</script></span>. </p>
<p>Now let me implement the training (<code>fit</code>) method. The cost function is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
cost(\boldsymbol\beta)=\frac1m\sum_{i=1}^m[y_i-\sigma(\boldsymbol{\beta}^\top\hat{\boldsymbol{x}}_i)]^2+\gamma ||\boldsymbol{\beta}||_\text{1 or 2}
</div>
<script type="math/tex; mode=display">
cost(\boldsymbol\beta)=\frac1m\sum_{i=1}^m[y_i-\sigma(\boldsymbol{\beta}^\top\hat{\boldsymbol{x}}_i)]^2+\gamma ||\boldsymbol{\beta}||_\text{1 or 2}
</script>
</div>
<p>I use gradient descent to decrease the cost function and find the optimal <span class="arithmatex"><span class="MathJax_Preview">\boldsymbol\beta</span><script type="math/tex">\boldsymbol\beta</script></span>. According to the (3.30), the gradient function is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\nabla_\boldsymbol{\beta}l(\boldsymbol\beta)=\frac{\partial l(\boldsymbol\beta)}{\partial \boldsymbol\beta}=\sum_{i=1}^m[y_i-\sigma(\boldsymbol{\beta}^\top\hat{\boldsymbol{x}}_i)]\hat{\boldsymbol{x}}_i
</div>
<script type="math/tex; mode=display">
\nabla_\boldsymbol{\beta}l(\boldsymbol\beta)=\frac{\partial l(\boldsymbol\beta)}{\partial \boldsymbol\beta}=\sum_{i=1}^m[y_i-\sigma(\boldsymbol{\beta}^\top\hat{\boldsymbol{x}}_i)]\hat{\boldsymbol{x}}_i
</script>
</div>
<p>In each iteration, if the absolute value of the increment of the cost function is less than the parameter <code>tol</code>, the iteration will break, which means the training is done. Otherwise the <span class="arithmatex"><span class="MathJax_Preview">\boldsymbol\beta</span><script type="math/tex">\boldsymbol\beta</script></span> should update according to</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\boldsymbol\beta←\boldsymbol\beta+\frac\alpha m\nabla_\boldsymbol{\beta}l(\boldsymbol\beta)
</div>
<script type="math/tex; mode=display">
\boldsymbol\beta←\boldsymbol\beta+\frac\alpha m\nabla_\boldsymbol{\beta}l(\boldsymbol\beta)
</script>
</div>
<p>Here <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> is the parameter <code>lr</code>. The training has a maximum number of iteration which is set by the parameter <code>max_iter</code>. One more thing, in order to draw a loss curve, the <code>fit</code> method will return an array of values of cost function during the training process.</p>
<p>The last thing is implementation of <code>predict</code> method. It will use the trained <span class="arithmatex"><span class="MathJax_Preview">\boldsymbol\beta</span><script type="math/tex">\boldsymbol\beta</script></span> to caculate a result for each line in X_test. The result will be caculated according to</p>
<div class="arithmatex">
<div class="MathJax_Preview">
p(\hat{\boldsymbol x}_i)=\sigma(\boldsymbol{\beta}^\top\hat{\boldsymbol{x}}_i)
</div>
<script type="math/tex; mode=display">
p(\hat{\boldsymbol x}_i)=\sigma(\boldsymbol{\beta}^\top\hat{\boldsymbol{x}}_i)
</script>
</div>
<p>And the <code>predict</code> method will return an array of results from all test lines.</p>
<p><strong>Step3</strong>. Train and test in framework</p>
<p>Now I have the split dataset (<code>X_train</code>, <code>X_test</code>, <code>y_train</code>, <code>y_test</code>) and LR class. Simply call the <code>fit</code> method on <code>X_train</code> and <code>y_train</code>. And using matplotlib to illustrate the loss curve according to the return value of <code>fit</code>.</p>
<p>In the test process simply call the <code>predict</code> method on <code>X_test</code> and get its return value. Then compare each element of it and its corresponding label in <code>y_test</code>. In this process I will use a confusion matrix to display the result of the trained model running on the test dataset.</p>
<p><strong>Step4</strong>. Using different parameters</p>
<p>In order to get the comparation table, I repeat the steps above using one different parameter each time, using accuracy <span class="arithmatex"><span class="MathJax_Preview">\left(\displaystyle\frac{TP+FN}{TP+TN+FP+FN}\right)</span><script type="math/tex">\left(\displaystyle\frac{TP+FN}{TP+TN+FP+FN}\right)</script></span> as the metric. For <code>lr</code>, there will be a extra metric: trend of the loss curve.</p>
<h2 id="result">Result</h2>
<h3 id="loss-curve">Loss curve</h3>
<p>The loss curve (one training process) of training process is as follows. Here the y-axis is the value of cost function, the x-axis is the number of iterations.</p>
<p><img src="Lab1_%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92.assets/image-20221008101412662.png" alt="image-20221008101412662" style="zoom:50%;" /></p>
<h3 id="comparation-table">Comparation table</h3>
<p>The original parameters I picked is:</p>
<ul>
<li>Positive and negative cases ratio in test dataset <span class="arithmatex"><span class="MathJax_Preview">(PNR)</span><script type="math/tex">(PNR)</script></span>:    1:1</li>
<li>Ratio of training set data to all <span class="arithmatex"><span class="MathJax_Preview">(TAR)</span><script type="math/tex">(TAR)</script></span>:   50%</li>
<li>Learning rate <span class="arithmatex"><span class="MathJax_Preview">(lr)</span><script type="math/tex">(lr)</script></span>:    0.1</li>
</ul>
<blockquote>
<p>Define some varibles as follows:</p>
<table>
<thead>
<tr>
<th align="center">mark</th>
<th align="center">meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><span class="arithmatex"><span class="MathJax_Preview">c_0</span><script type="math/tex">c_0</script></span></td>
<td align="center">the number of negative cases of the dataset</td>
</tr>
<tr>
<td align="center"><span class="arithmatex"><span class="MathJax_Preview">c_1</span><script type="math/tex">c_1</script></span></td>
<td align="center">the number of positive cases of the dataset</td>
</tr>
<tr>
<td align="center"><span class="arithmatex"><span class="MathJax_Preview">t_0</span><script type="math/tex">t_0</script></span></td>
<td align="center">the number of negative cases of X_train</td>
</tr>
<tr>
<td align="center"><span class="arithmatex"><span class="MathJax_Preview">t_1</span><script type="math/tex">t_1</script></span></td>
<td align="center">the number of positive cases of X_train</td>
</tr>
</tbody>
</table>
<p>The four variables above satisfy the following formulas:
$$
TAR=\frac{t_0+t_1}{c_0+c_1}\quad\quad PNR=\frac{t_1}{t_0}
$$
Given certain TAR and PNR, we can get
$$
t_0=\frac{TAR\times(c_0+c_1)}{1+PNR}\quad\quad t_1=PNR\times t_0
$$
The results above make it easier to change the parameter TAR and PNR when watching their effect of training, which enables us to be free of caculating the size of training set. But one thing to be noticed is that 
$$
\textbf{assert } t_1 &lt;= c_1\textbf{ and }t_0 &lt;= c_0
$$</p>
</blockquote>
<p>Then each time I will use one difference parameter on the basis of these values.</p>
<hr />
<h4 id="pnr">PNR</h4>
<p><img src="Lab1_%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92.assets/image-20221009004226081.png" alt="image-20221009004226081" style="zoom:50%;" /></p>
<table>
<thead>
<tr>
<th align="center">PNR</th>
<th align="center">1:1</th>
<th align="center">1.3:1</th>
<th align="center">1.6:1</th>
<th align="center">1.9:1</th>
<th align="center">2.2:1</th>
<th align="center">2.5:1</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Accuracy</td>
<td align="center">63.76%</td>
<td align="center">79.75%</td>
<td align="center">80.87%</td>
<td align="center">81.73%</td>
<td align="center">80.37%</td>
<td align="center">79.75%</td>
</tr>
</tbody>
</table>
<p>Other: <span class="arithmatex"><span class="MathJax_Preview">TAR=0.5,lr=0.1,tol=10^{-5}</span><script type="math/tex">TAR=0.5,lr=0.1,tol=10^{-5}</script></span>    Each <span class="arithmatex"><span class="MathJax_Preview">PNR</span><script type="math/tex">PNR</script></span> case repeated 10 times, The result is the average.</p>
<hr />
<h4 id="tar">TAR</h4>
<p><img src="Lab1_%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92.assets/image-20221009003342744.png" alt="image-20221009003342744" style="zoom:50%;" /></p>
<table>
<thead>
<tr>
<th align="center">TAR</th>
<th align="center">20%</th>
<th align="center">30%</th>
<th align="center">40%</th>
<th align="center">50%</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Accuracy</td>
<td align="center">64.73%</td>
<td align="center">69.73%</td>
<td align="center">63.27%</td>
<td align="center">62.68%</td>
</tr>
</tbody>
</table>
<p>Other: <span class="arithmatex"><span class="MathJax_Preview">PNR=1:1,lr=0.1,tol=10^{-5}</span><script type="math/tex">PNR=1:1,lr=0.1,tol=10^{-5}</script></span>    Each <span class="arithmatex"><span class="MathJax_Preview">TAR</span><script type="math/tex">TAR</script></span> case repeated 10 times, The result is the average.</p>
<hr />
<h4 id="lr">lr</h4>
<p><img src="Lab1_%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92.assets/image-20221009005524730.png" alt="image-20221009005524730" style="zoom:50%;" /></p>
<table>
<thead>
<tr>
<th align="center">lr</th>
<th align="center">0.05</th>
<th align="center">0.1</th>
<th align="center">0.2</th>
<th align="center">0.4</th>
<th align="center">0.8</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Accuracy</td>
<td align="center">62.38%</td>
<td align="center">62.21%</td>
<td align="center">60.08%</td>
<td align="center">65.94%</td>
<td align="center">58.58%</td>
</tr>
</tbody>
</table>
<p>Other: <span class="arithmatex"><span class="MathJax_Preview">PNR=1:1,TAR=0.5,tol=10^{-5}</span><script type="math/tex">PNR=1:1,TAR=0.5,tol=10^{-5}</script></span>    Each <span class="arithmatex"><span class="MathJax_Preview">lr</span><script type="math/tex">lr</script></span> case repeated 10 times, The result is the average.</p>
<p><img src="Lab1_%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92.assets/image-20221009005950272.png" alt="image-20221009005950272" style="zoom: 50%;" /></p>
<p>The loss curve for each case of <span class="arithmatex"><span class="MathJax_Preview">lr</span><script type="math/tex">lr</script></span>. We can intuitively conclude that the larger the value of <span class="arithmatex"><span class="MathJax_Preview">lr</span><script type="math/tex">lr</script></span>, the faster the loss function decreases.</p>
<h3 id="best-accuracy">Best accuracy</h3>
<p>From the results above, the best accuracy is 81.73%, which has these parameters:
$$
PNR=1.9:1\quad TAR=0.5\quad lr=0.1\quad tol=10^{-5}
$$
Its confusion matrix is</p>
<p><img src="Lab1_%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92.assets/image-20221009010633455.png" alt="image-20221009010633455" style="zoom:50%;" /></p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav">
      <a href="../3_%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../3_%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/" class="btn btn-xs btn-link">
        3 线性判别分析
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>